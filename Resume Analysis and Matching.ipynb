{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g72UqDfD2ck",
        "outputId": "f9c8c904-7253-49f7-e0ba-645480f383b3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZRUFN7lBtmZ",
        "outputId": "493b0326-50c8-4c4e-f07e-a0239edb73a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank 1: resume 3.pdf, Similarity Score: 0.16320139082325497\n",
            "Rank 2: resume 1.pdf, Similarity Score: 0.1568605939388511\n",
            "Rank 3: resume 2.pdf, Similarity Score: 0.1557121520926502\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pdfplumber\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        text = \"\"\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    words = [word for word in tokens if word.isalnum()]\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "    return \" \".join(lemmatized_words)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    job_description = \"Software Engineer with experience in Bootstrap\"\n",
        "\n",
        "\n",
        "    resumes = [\n",
        "        \"resume 1.pdf\",\n",
        "        \"resume 2.pdf\",\n",
        "        \"resume 3.pdf\",\n",
        "\n",
        "    ]\n",
        "\n",
        "    preprocessed_job_desc = preprocess_text(job_description)\n",
        "\n",
        "    candidate_scores = []\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "    for resume_path in resumes:\n",
        "        resume_text = extract_text_from_pdf(resume_path)\n",
        "        preprocessed_resume = preprocess_text(resume_text)\n",
        "\n",
        "\n",
        "        corpus = [preprocessed_resume, preprocessed_job_desc]\n",
        "        tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "\n",
        "        similarity_matrix = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
        "        similarity_score = similarity_matrix[0][0]\n",
        "\n",
        "        candidate_scores.append((resume_path, similarity_score))\n",
        "\n",
        "\n",
        "    candidate_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "    for rank, (resume_path, score) in enumerate(candidate_scores, 1):\n",
        "        print(f\"Rank {rank}: {resume_path}, Similarity Score: {score}\")\n"
      ]
    }
  ]
}